# -*- coding: utf-8 -*-
"""MLTerapan_SistemRekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WAz4zAC17WV_leKSEzvxHBAYPy3VmqoL

Machine Learning Terapan - Proyek Akhir : Sistem Rekomendasi
- Nama : Attiya Dianti Fadli
- ID : MC189D5X0806

# 1. Import Library yang Dibutuhkan
"""

!pip install -q kaggle

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
from google.colab import files
from sklearn.preprocessing import LabelEncoder

"""mengimport semua library yang dibutuhkan dalam proyek

# 2. Data Understanding

data diambil dari [Kaggle](https://www.kaggle.com/datasets/aprabowo/indonesia-tourism-destination)
"""

files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d aprabowo/indonesia-tourism-destination

!unzip indonesia-tourism-destination.zip

"""terdapat empat file csv. namun, untuk proyek ini hanya menggunakan `tourism_rating.csv` dan `tourism_with_id.csv`"""

!mv '/content/tourism_with_id.csv' 'data.csv'
!mv '/content/tourism_rating.csv' 'rating.csv'

#Menghapus file yang tidak dibutuhkan
!rm indonesia-tourism-destination.zip
!rm package_tourism.csv
!rm user.csv

"""mengubah nama file agar lebih mudah digunakan dan menghapus file yang tidak dibutuhkan

- `tourism_rating.csv` — Berisi data rating destinasi pariwisata oleh pengguna.
- `tourism_with_id.csv` — Berisi informasi destinasi dengan ID unik yang dapat digunakan untuk relasi atau referensi.
"""

data = pd.read_csv('data.csv')
rating = pd.read_csv('rating.csv')

"""membaca dan menyimpan dataset dalam variabel"""

data.info()

"""menampilkan informasi dari variabel data
- Total baris ada 437
- Total kolom ada 13
- Terdapat 8 kolom bertipe data numerik
- Terdapat 5 kolom bertipe data object
"""

rating.info()

"""menampilkan informasi dari variabel rating
- Total baris ada 10000
- Total kolom ada 3
- Semua kolom bertipe numerik
"""

data.head()

"""menampilkan 5 data teratas dari variabel data"""

rating.head()

"""menampilkan 5 data teratas dari variabel rating

# 3. Univariate Exploratory Data Analysis

1. data:
- `Place_Id`: ID unik untuk setiap destinasi wisata.
- `Place_Name`: Nama tempat atau destinasi wisata.
- `Description`: Deskripsi singkat mengenai tempat wisata tersebut.
- `Category`: Kategori wisata (misalnya alam, budaya, sejarah, dll).
- `City`: Kota tempat destinasi tersebut berada.
- `Price`: Estimasi biaya tiket masuk atau biaya kunjungan (dalam Rupiah).
- `Rating`: Rating rata-rata destinasi berdasarkan penilaian pengunjung (skala biasanya 1–5).
- `Time_Minutes`: Estimasi durasi kunjungan (dalam menit); nilai bisa kosong pada beberapa tempat.
- `Coordinate`: Lokasi geografis gabungan dalam bentuk teks (biasanya latitude,longitude).
- `Lat`: Latitude dari lokasi tempat wisata.
- `Long`: Longitude dari lokasi tempat wisata.
- `Unnamed`: 11: Kolom kosong
- `Unnamed`: 12: Kolom kosong

2. rating
- `User_Id`: ID unik untuk setiap pengguna.
- `Place_Id`: ID tempat wisata yang dirating (mengacu pada Place_Id di tourism_with_id.csv).
- `Place_Ratings`: Nilai rating yang diberikan pengguna terhadap tempat tertentu (skala biasanya 1–5).

**Variabel Data**
"""

data.head(1)

"""menampilkan 1 kolom dari variabel data"""

# Hitung jumlah nama tempat unik
print('Jumlah Nama Tempat Unik:', data['Place_Name'].nunique())
place_id_name = data.drop_duplicates(subset=['Place_Id'])[['Place_Id', 'Place_Name']].set_index('Place_Id')

# Hitung jumlah kemunculan masing-masing Place_Id
place_id_counts = data['Place_Id'].value_counts()
place_summary = place_id_name.join(place_id_counts.rename('Jumlah'))
place_summary = place_summary.sort_index()
place_summary = place_summary[['Place_Name', 'Jumlah']]
place_summary = place_summary.reset_index()
print(place_summary.to_string(index=False))

"""total nama tempat unik: 437 <br> Data menunjukkan keragaman tempat wisata dengan fokus pada pengalaman yang unik di setiap lokasi."""

print('Jumlah Kategori Wisata:', data['Category'].nunique())
category_counts = data['Category'].value_counts()
print(category_counts)

#Menampilkan dalam bentuk grafik
plt.figure(figsize=(5, 3))
sns.countplot(data=data, x=data['Category'])
plt.xticks(rotation=90)
plt.show()

"""jumlah kunjungan ke berbagai jenis tempat. Kategori Budaya mencatat jumlah kunjungan tertinggi, yaitu 130, diikuti oleh Taman Hiburan dengan total 120 kunjungan. Selanjutnya, Cagar Alam menunjukkan angka yang lebih rendah, sekitar 70 kunjungan, sementara Bahari memiliki 40 kunjungan. Terakhir, Pusat Pembelajaran dan Tempat Ibadah masing-masing hanya mendapatkan sekitar 20 kunjungan, yang merupakan jumlah terendah di antara semua kategori."""

print('Jumlah Daerah:', data['City'].nunique())
city_counts = data['City'].value_counts()
print(city_counts)

#Menampilkan dalam bentuk grafik
plt.figure(figsize=(5, 3))
sns.countplot(data=data, x=data['City'])
plt.xticks(rotation=90)
plt.show()

""" jumlah kunjungan ke beberapa kota. Kategori Yogyakarta mencatat jumlah kunjungan tertinggi, mencapai 130. Diikuti oleh Bandung dengan total 120 kunjungan. Jakarta juga memiliki angka yang signifikan, sekitar 90 kunjungan. Sementara itu, Semarang mencatat sekitar 60 kunjungan, dan Surabaya memiliki jumlah terendah dengan sekitar 50 kunjungan."""

print('Harga minimum:', data['Price'].min())
print('Harga maksimum:', data['Price'].max())

price_counts = data['Price'].value_counts()
print(price_counts)

"""Ringkasan Statistik
- Rentang Harga: Rp 0 - Rp 900.000
- Harga minimum: 0
- Harga maksimum: 900000
"""

print('Jumlah Durasi Wisata:', data['Time_Minutes'].nunique())
time_counts = data['Time_Minutes'].value_counts()
print(time_counts)

#Menampilkan dalam bentuk grafik
plt.figure(figsize=(10, 3))
sns.countplot(data=data, x=data['Time_Minutes'])
plt.xticks(rotation=90)
plt.show()

"""distribusi durasi wisata dalam satuan menit. Durasi sekitar 90 menit memiliki jumlah kunjungan tertinggi, mencapai 40. Kategori durasi 60 menit juga menunjukkan angka yang signifikan, dengan sekitar 35 kunjungan. Selain itu, durasi 45 menit dan 120 menit masing-masing mencatat sekitar 30 dan 25 kunjungan. Sementara itu, durasi yang lebih pendek, seperti 10 menit dan 15 menit, memiliki jumlah kunjungan yang jauh lebih rendah, menunjukkan bahwa pengunjung cenderung memilih durasi yang lebih lama untuk pengalaman wisata mereka."""

print('Jumlah koordinat unik:', data['Coordinate'].nunique())
koor_counts = data['Coordinate'].value_counts()
print(koor_counts)

"""data menunjukkan **437 lokasi unik** dengan distribusi merata (1 entri per koordinat). Sebaran terpusat di **Jakarta-Bogor** area (lat -6.1 sampai -6.3, lng ~106.8) dengan beberapa titik di **Surabaya** dan **Singapura**. Pola ini mengindikasikan **cakupan geografis luas** namun **tidak ada clustering** - kemungkinan strategi distribusi merata atau early-stage expansion."""

print('Range Latitude:', data['Lat'].min(), 'sampai', data['Lat'].max())
print('Range Longitude:', data['Long'].min(), 'sampai', data['Long'].max())

data['Lat_Bin'] = pd.cut(data['Lat'], bins=10)
lat_counts = data['Lat_Bin'].value_counts().sort_index()
plt.figure(figsize=(10, 4))
lat_counts.plot(kind='bar', color='skyblue')
plt.title('Distribusi Destinasi Berdasarkan Latitude')
plt.xlabel('Rentang Latitude')
plt.ylabel('Jumlah Destinasi')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""distribusi jumlah destinasi berdasarkan rentang latitude. Rentang (-8.207, -7.727) mencatat jumlah destinasi tertinggi, mencapai lebih dari 200. Rentang (-7.727, -6.343) juga menunjukkan angka yang signifikan, dengan sekitar 150 destinasi. Selanjutnya, rentang (-6.343, -5.453) memiliki jumlah destinasi yang lebih rendah, sekitar 100. Rentang (-5.453, -4.487) dan (-4.487, -3.628) masing-masing mencatat jumlah yang lebih sedikit, sementara rentang terakhir (-1.704, -0.511) menunjukkan jumlah destinasi terendah.

**Variabel Rating**
"""

review_per_user = rating.groupby('User_Id').size()
print("Jumlah review per user:")
print(review_per_user)

"""data menunjukkan 300 user dengan aktivitas review yang sangat konsisten - mayoritas user memberikan 26-39 review per orang (rata-rata ~30 review). Distribusi yang merata ini mengindikasikan high engagement dan kemungkinan adanya sistem incentive atau requirement minimum untuk review, karena jarang terjadi pola seragam seperti ini secara natural di platform user-generated content."""

place_review_counts = rating['Place_Id'].value_counts()

# Cetak jumlah berdasarkan urutan Place_Id
print("\nJumlah kemunculan per Place_Id (urut berdasarkan ID):\n", place_review_counts.sort_index())

# Statistik jumlah review
min_count = place_review_counts.min()
max_count = place_review_counts.max()

min_place_id = place_review_counts[place_review_counts == min_count].index.tolist()
max_place_id = place_review_counts[place_review_counts == max_count].index.tolist()

print(f"\nPlace_Id dengan terdikit: {min_place_id}")
print(f"Place_Id dengan terbanyak: {max_place_id}")

# Ambil 20 Place_Id teratas berdasarkan ID (bukan jumlah review)
top_20_places = place_review_counts.sort_index().head(20)

# Plot barplot dengan Place_Id di sumbu X dan jumlah review di sumbu Y
plt.figure(figsize=(10, 3))
sns.barplot(x=top_20_places.index, y=top_20_places.values)
plt.xticks(rotation=90)
plt.xlabel('Place_Id')
plt.ylabel('Jumlah Review')
plt.tight_layout()
plt.show()

"""distribusi kemunculan berdasarkan Place_Id yang diurutkan berdasarkan ID. Place_Id 8 mencatat jumlah review tertinggi, mencapai sekitar 30. Beberapa Place_Id lain, seperti 7 dan 17, juga menunjukkan angka yang signifikan dengan sekitar 25 review. Sebagian besar ID lainnya memiliki jumlah review yang relatif seimbang, berkisar antara 10 hingga 20. Secara keseluruhan, grafik ini mencerminkan variasi dalam jumlah review untuk setiap tempat."""

print('Jumlah total rating:', rating['Place_Ratings'].count())
print('Rating unik:', rating['Place_Ratings'].unique())
print('Distribusi rating:\n', rating['Place_Ratings'].value_counts().sort_index())
print('Rata-rata rating:', rating['Place_Ratings'].mean())

#Menampilkan dalam bentuk grafik
plt.figure(figsize=(5, 3))
sns.countplot(data=rating, x=rating['Place_Ratings'])
plt.xticks(rotation=90)
plt.show()

"""distribusi rating berdasarkan Place_Ratings. Rating 2 tercatat sebagai yang tertinggi, dengan jumlah sekitar 2000. Rating 3, 4, dan 5 juga menunjukkan angka yang signifikan, masing-masing berada di kisaran yang sama. Namun, rating 1 memiliki jumlah yang jauh lebih rendah dibandingkan dengan rating lainnya. Secara keseluruhan, grafik ini mencerminkan bahwa sebagian besar rating berkisar di antara 2 hingga 5, dengan sedikit penilaian yang sangat rendah.

# 4. Data Preprocessing

menggabungkan dataset variabel data dan variabel rating
"""

# Gabungkan data rating dengan informasi tempat berdasarkan 'Place_Id'
merged_data = pd.merge(rating, data, on='Place_Id')

# Tampilkan hasil gabungan
merged_data.head()

"""menghapus kolom yang tidak di perlukan dalam dataset"""

# Hapus kolom-kolom yang tidak relevan
merged_data = merged_data.drop(columns=['City', 'Price', 'Time_Minutes', 'Coordinate', 'Lat', 'Long', 'Unnamed: 11', 'Unnamed: 12', 'Lat_Bin'])

# Tampilkan hasil setelah kolom dihapus
merged_data.head()

"""pada proyek ini hanya menggunakan kolom `User_Id`, `Place_Id`, `Place_Ratings`, `Place_Name`, `Description`, `Category`, dan `Rating`

mengecek missing value
"""

merged_data.isnull().sum()

"""tidak terdapat missing value

menampilkan dataset yang sudah dibersihkan
"""

dataset = merged_data
dataset

"""# 5. Data Preparation

menghapus data duplikat berdasarkan Place_Id
"""

dataset = dataset.drop_duplicates('Place_Id')

"""melakukan konversi data Series menjadi list"""

# Konversi beberapa kolom dari Series menjadi list
place_id_list = dataset['Place_Id'].tolist()
place_name_list = dataset['Place_Name'].tolist()
user_id_list = dataset['User_Id'].tolist()
place_rating_list = dataset['Place_Ratings'].tolist()

# Tampilkan panjang dari setiap list
print(len(place_id_list))
print(len(place_name_list))
print(len(user_id_list))
print(len(place_rating_list))

"""konsistensi data sempurna - semua kolom memiliki 437 records tanpa missing values, mengindikasikan dataset yang clean dan well-structured. Konversi ke list format ini biasanya dilakukan untuk persiapan modeling (seperti recommendation system) atau data processing yang membutuhkan struktur array/list dibanding DataFrame."""

# Pilih kolom bertipe object (biasanya berisi teks)
text_columns = dataset.select_dtypes(include='object').columns.tolist()

# Tambahkan kolom 'Place_Id' secara manual jika belum ada
if 'Place_Id' not in text_columns:
    text_columns.append('Place_Id')

# Buat dictionary dari kolom yang dipilih (konversi ke string untuk jaga-jaga)
place_text_dict = {col: dataset[col].astype(str).tolist() for col in text_columns}

# Buat DataFrame baru dari dictionary
place_text_df = pd.DataFrame(place_text_dict)

# Pastikan 'Place_Id' menjadi kolom pertama
cols = ['Place_Id'] + [col for col in place_text_df.columns if col != 'Place_Id']
place_text_df = place_text_df[cols]

# Tampilkan hasil
print("Kolom teks yang digunakan:", cols)
place_text_df.head()

"""4 kolom text features: Place_Id, Place_Name, Description, dan Category yang akan digunakan untuk content-based filtering. Data mencakup berbagai kategori wisata (Budaya, Bahari, Taman Hiburan) dengan deskripsi lengkap dalam bahasa Indonesia dan Jawa, mengindikasikan dataset tourism places yang rich in content untuk recommendation modeling.

# 6. Model Development Dengan Content Based Filtering

Melakukan assign dataframe ke variabel baru yaitu data.
"""

data = place_text_df

"""Membangun sistem rekomendasi."""

tfidf = TfidfVectorizer()

# Latih model vektor pada data dengan kolom category
tfidf.fit(data['Category'])

# Tampilkan nama-nama fitur hasil vektorisasi
fitur_tfidf = tfidf.get_feature_names_out()
print("Fitur hasil ekstraksi:", fitur_tfidf[:10])

"""melakukan fit dan transformasi ke TF-IDF matrix"""

# Transformasi teks gabungan ke dalam representasi vektor TF-IDF
matriks_tfidf = tfidf.transform(data['Category'])

# Tampilkan dimensi matriks hasil transformasi
print("Ukuran matriks TF-IDF:", matriks_tfidf.shape)

"""membuat vektor TF-IDF dalam bentuk matriks"""

matriks_tfidf.todense()

"""membuat matriks TF-IDF untuk beberapa model dan category"""

# Konversi TF-IDF matrix ke dalam DataFrame untuk ditampilkan
df_tfidf = pd.DataFrame(
    matriks_tfidf.toarray(),
    columns=tfidf.get_feature_names_out(),
    index=place_text_df['Category']
)

# Tampilkan secara acak 10 baris & 10 kolom agar tidak tampil penuh
sampel_tfidf = df_tfidf.sample(n=10, axis=0).sample(n=10, axis=1)
print("Matriks TF-IDF:")
sampel_tfidf

"""Menghitung derajat kesamaan (similarity degree) teknik cosine similarity."""

# Hitung cosine similarity antar baris pada matriks TF-IDF
kemiripan_tempat = cosine_similarity(matriks_tfidf)

# Konversi hasil similarity ke bentuk DataFrame agar mudah dibaca
df_kemiripan = pd.DataFrame(
    kemiripan_tempat,
    index=place_text_df['Place_Name'],
    columns=place_text_df['Category']
)

# Tampilkan sebagian kecil hasil
print("Contoh nilai kesamaan antar tempat wisata:")
df_kemiripan.iloc[:5, :5]

"""membuat model rekomendasi

"""

# Ambil kolom kategori dan ubah ke string
kategori_teks = place_text_df['Category'].fillna('').astype(str)

# Buat vectorizer dan fit-transform
vector_kategori = TfidfVectorizer()
tfidf_kategori = vector_kategori.fit_transform(kategori_teks)

# Hitung similarity matrix berdasarkan kategori
similarity_kategori = cosine_similarity(tfidf_kategori)

# Buat fungsi rekomendasi berdasarkan nama tempat
def model_recommendations(nama_tempat, top_n=5):
    if nama_tempat not in place_text_df['Category'].values:
        print(f"'{nama_tempat}' tidak ditemukan dalam data.")
        return []

    # Temukan indeks tempat input
    idx = place_text_df[place_text_df['Category'] == nama_tempat].index[0]

    # Ambil skor similarity untuk tempat tersebut
    skor_kemiripan = list(enumerate(similarity_kategori[idx]))

    # Urutkan berdasarkan skor tertinggi (kecuali dirinya sendiri)
    rekomendasi = sorted(skor_kemiripan, key=lambda x: x[1], reverse=True)
    rekomendasi = [i for i in rekomendasi if i[0] != idx][:top_n]

    # Ambil nama tempat dari indeks hasil rekomendasi
    hasil = place_text_df.iloc[[i[0] for i in rekomendasi]]['Place_Name'].tolist()

    return hasil

"""melihat rekomendasi wisata untuk category taman hiburan"""

model_recommendations('Taman Hiburan')

"""model ini merekomendasikan
`Tugu Pal Putih Jogja`,
 `Surabaya North Quay`,
 `Grand Maerakaca`,
 `Taman Cattleya`,
 `Taman Pintar Yogyakarta` untuk categort `Taman Hiburan`

melihat rekomendasi wisata untuk category budaya
"""

model_recommendations('Budaya')

"""model ini merekomendasikan
`Museum Kereta Ambarawa`,
 `Kampung Wisata Sosro Menduran`,
 `Museum Gedung Sate`,
 `Museum Taman Prasasti`,
 `De Mata Museum Jogja` untuk categort `Budaya`

# 7. Model Model Development dengan Collaborative Filtering

Menyimpan dataset rating yang sudah pernah diread di variabel df.
"""

df = rating
df

"""Menyandikan (encode) kolom `User_Id`"""

# Inisialisasi encoder
user_encoder = LabelEncoder()

# Terapkan encoding ke kolom user_id
dataset['user_id_enc'] = user_encoder.fit_transform(dataset['User_Id'])

# Tampilkan hasil encoding
dataset[['User_Id', 'user_id_enc']].drop_duplicates().head()

"""Menyandikan (encode) kolom `Place_Id`"""

# Inisialisasi encoder
place_encoder = LabelEncoder()

# Terapkan encoding ke kolom user_id
dataset['place_id_enc'] = place_encoder.fit_transform(dataset['Place_Id'])

# Tampilkan hasil encoding
dataset[['Place_Id', 'place_id_enc']].drop_duplicates().head()

"""Menyandikan (encode) kolom `Place_Ratings`"""

# Inisialisasi encoder
rating_encoder = LabelEncoder()

# Terapkan encoding ke kolom user_id
dataset['rating_id_enc'] = rating_encoder.fit_transform(dataset['Place_Ratings'])

# Tampilkan hasil encoding
dataset[['Place_Ratings', 'rating_id_enc']].drop_duplicates().head()

"""Menampilkan jumlah user dan cellphone dan menampilkan nilai minimum dan maximum dari rating."""

# Jumlah unik pengguna dan tempat wisata
total_user = dataset['User_Id'].nunique()
total_tempat = dataset['Place_Id'].nunique()

print(f"Jumlah pengguna unik      : {total_user}")
print(f"Jumlah tempat wisata unik : {total_tempat}")

# Nilai minimum dan maksimum dari rating
nilai_min = dataset['Place_Ratings'].min()
nilai_maks = dataset['Place_Ratings'].max()

print(f"Rating terendah  : {nilai_min}")
print(f"Rating tertinggi : {nilai_maks}")

"""Mengacak dataset"""

# Ambil semua kolom kecuali yang mengandung '_enc'
kolom_asli = [col for col in dataset.columns if not col.endswith('_enc')]

# Tampilkan hanya kolom asli
dataset[kolom_asli].head()

"""menggabungkan kolom user dan cellphone ke dalam array x, serta membuat array y untuk rating yang telah dinormalisasi, lalu membaginya menjadi train dan validasi"""

# Ambil pasangan user dan place sebagai array fitur menggunakan encoded IDs
fitur_interaksi = dataset[['user_id_enc', 'place_id_enc']].values

# Normalisasi nilai rating (0–1)
min_nilai = dataset['Place_Ratings'].min()
max_nilai = dataset['Place_Ratings'].max()

nilai_rating = dataset['Place_Ratings'].apply(
    lambda skor: (skor - min_nilai) / (max_nilai - min_nilai)
).values

# Tentukan batas index 80%
batas_split = int(0.8 * len(dataset))

# Bagi ke dalam data pelatihan dan validasi
x_train, x_valid = fitur_interaksi[:batas_split], fitur_interaksi[batas_split:]
y_train, y_valid = nilai_rating[:batas_split], nilai_rating[batas_split:]

# Cetak sebagian kecil hasil
print("Contoh fitur gabungan (user_enc, place_enc):\n", x_train[:5]) # Tampilkan 5 baris pertama saja
print("Contoh rating terstandarisasi:\n", y_train[:5]) # Tampilkan 5 baris pertama saja

"""membuat model rekomendasi"""

import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Embedding, Flatten, Dot, Dense, Dropout, Concatenate

class RecommenderNet(Model):
    def __init__(self, total_user, total_place, dim_embedding=50):
        super(RecommenderNet, self).__init__()

        # Embedding layer untuk user
        self.user_embedding = Embedding(
            input_dim=total_user,
            output_dim=dim_embedding,
            embeddings_initializer="he_normal",
            embeddings_regularizer=tf.keras.regularizers.l2(1e-6)
        )

        # Embedding layer untuk place
        self.place_embedding = Embedding(
            input_dim=total_place,
            output_dim=dim_embedding,
            embeddings_initializer="he_normal",
            embeddings_regularizer=tf.keras.regularizers.l2(1e-6)
        )

        # Lapisan tambahan (opsional)
        self.dot = Dot(axes=1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        place_vector = self.place_embedding(inputs[:, 1])

        # Bisa langsung pakai dot product untuk rating prediksi
        hasil = self.dot([user_vector, place_vector])

        return hasil

"""menginisialisasi model RecommenderNet"""

# Hitung jumlah unik user dan tempat
jumlah_user = dataset['user_id_enc'].nunique()
jumlah_place = dataset['place_id_enc'].nunique()

# Inisialisasi model
model_rekomendasi = RecommenderNet(
    total_user=jumlah_user,
    total_place=jumlah_place,
    dim_embedding=50
)

# Compile model
model_rekomendasi.compile(
    loss='mean_squared_error',
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=['mae']
)

"""melakukan training terhadap model RecommenderNet yang sudah diinisialisasi dan dikompilasi"""

# Melatih model
history = model_rekomendasi.fit(
    x=x_train,
    y=y_train,
    validation_data=(x_valid, y_valid),
    epochs=20,
    batch_size=64,
    verbose=1
)

"""menampilkan grafik proses training"""

plt.figure(figsize=(8, 5))

# Plot Loss
plt.plot(history.history['loss'], label='Train Loss (MSE)', linestyle='-', marker='o')
plt.plot(history.history['val_loss'], label='Val Loss (MSE)', linestyle='--', marker='x')

# Plot MAE
plt.plot(history.history['mae'], label='Train MAE', linestyle='-', marker='s')
plt.plot(history.history['val_mae'], label='Val MAE', linestyle='--', marker='d')

# Tambahan styling
plt.title('Grafik Proses Training: Loss dan MAE')
plt.xlabel('Epoch')
plt.ylabel('Nilai')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""- Train Loss (MSE): Terlihat menurun secara konsisten, menunjukkan bahwa model belajar dari data pelatihan dengan baik.
- Val Loss (MSE): Relatif konstan di sekitar 0.46, mengindikasikan bahwa model belum mampu meningkatkan performa pada data validasi.
- Train MAE: Menurun secara signifikan, menunjukkan penurunan rata-rata kesalahan absolut selama pelatihan.
- Val MAE: Stabil di sekitar 0.61, menandakan kemungkinan overfitting, karena model tidak memperbaiki kinerjanya terhadap data yang belum dilihat.

mendapatkan place yang belum direview oleh user
"""

# Salin data utama
# Menggunakan dataset yang sudah digabungkan dan diproses
place_df = dataset.copy()
rating_df = pd.read_csv('rating.csv')

# Buat mapping dari Place_Id asli ke Place_Id yang di-encode
place_to_place_encoded = {original_id: encoded_id for original_id, encoded_id in zip(dataset['Place_Id'], dataset['place_id_enc'])}

# Buat mapping dari User_Id asli ke User_Id yang di-encode
user_to_user_encoded = {original_id: encoded_id for original_id, encoded_id in zip(dataset['User_Id'], dataset['user_id_enc'])}

# Ambil salah satu user secara acak dari rating_df
# Menggunakan nama kolom yang benar 'User_Id'
sample_user = rating_df['User_Id'].sample(1).iloc[0]

# Tempat yang sudah pernah diulas user ini
# Menggunakan nama kolom yang benar 'User_Id' dan 'Place_Id'
reviewed_places = rating_df[rating_df['User_Id'] == sample_user]['Place_Id']

# Ambil daftar tempat yang belum diulas user dari place_df
# Menggunakan nama kolom yang benar 'Place_Id'
unrated_places = place_df[~place_df['Place_Id'].isin(reviewed_places)]['Place_Id']

# Filter hanya tempat yang ada dalam kamus encoded
unrated_places = list(
    set(unrated_places).intersection(set(place_to_place_encoded.keys()))
)

# Encode tempat-tempat yang belum diulas
encoded_unrated_places = [[place_to_place_encoded[p]] for p in unrated_places]

# Encode user ke bentuk integer
encoded_user = user_to_user_encoded.get(sample_user)

# Bentuk array gabungan user-tempat untuk prediksi
user_place_input = np.hstack(
    ([[encoded_user]] * len(encoded_unrated_places), encoded_unrated_places)
)

"""mendapatkan hasil rekomendasi tempat wisata"""

print(f"Menampilkan rekomendasi untuk user: {sample_user}")
print('===' * 9)

# Filter data review khusus untuk user ini, lalu ganti nama kolom supaya konsisten
place_reviewed_by_user = rating_df[rating_df['User_Id'] == sample_user].rename(columns={'Place_Id': 'place_id', 'Place_Ratings': 'rating'})

# Buat mapping dari place_id yang sudah di-encode ke place_id asli
place_encoded_to_place = {enc_id: orig_id for orig_id, enc_id in zip(dataset['Place_Id'], dataset['place_id_enc'])}

# Buat mapping dari place_id asli ke place_id yang sudah di-encode
place_id_to_encoded = dict(zip(dataset['Place_Id'], dataset['place_id_enc']))

# List tempat yang sudah pernah direview user (ID asli)
reviewed_places = place_reviewed_by_user['place_id'].unique()

# Ambil 5 tempat dengan rating tertinggi yang sudah pernah direview user
top_user_places = (
    place_reviewed_by_user.sort_values(by='rating', ascending=False)
    .head(5)
    .place_id.values
)

print("Tempat dengan rating tertinggi dari user:")
if len(top_user_places) == 0:
    print("User belum memberikan rating tinggi pada tempat manapun.")
else:
    for pid in top_user_places:
        name = place_df.loc[place_df['Place_Id'] == pid, 'Place_Name'].values
        if len(name) > 0:
            print(f"- {name[0]}")

print('----' * 8)

# Ambil semua tempat yang ada (dalam bentuk encoded place_id)
all_encoded_places = list(place_encoded_to_place.keys())

# Ambil tempat yang sudah pernah direview user (encoded place_id)
reviewed_encoded_places = [place_id_to_encoded[pid] for pid in reviewed_places if pid in place_id_to_encoded]

# Filter tempat yang belum pernah direview user (encoded place_id)
unrated_encoded_places = [p for p in all_encoded_places if p not in reviewed_encoded_places]

if not unrated_encoded_places:
    print("Tidak ada tempat baru untuk direkomendasikan.")
else:
    # Buat input array untuk model (user_id encoded dan place_id encoded)
    user_array = np.array([[encoded_user]] * len(unrated_encoded_places))
    place_array = np.array([[p] for p in unrated_encoded_places])
    user_place_array = np.hstack((user_array, place_array))

    # Prediksi rating model untuk tempat yang belum direview user
    ratings = model_rekomendasi.predict(user_place_array).flatten()

    # Ambil 10 tempat dengan rating prediksi tertinggi
    top_indices = ratings.argsort()[-10:][::-1]

    # Ambil place_id encoded hasil rekomendasi
    top_recommended_encoded = [unrated_encoded_places[i] for i in top_indices]

    # Konversi kembali ke place_id asli
    top_recommended_original = [place_encoded_to_place.get(enc) for enc in top_recommended_encoded]

    # Ambil data detail tempat rekomendasi dari place_df
    recommended_places = place_df[place_df['Place_Id'].isin(top_recommended_original)]

    print("Top 10 rekomendasi tempat wisata:")
    if recommended_places.empty:
        print("Tidak ditemukan rekomendasi tempat.")
    else:
        # Supaya urutan rekomendasi sesuai dengan hasil prediksi
        details_dict = recommended_places.set_index('Place_Id')['Place_Name'].to_dict()
        for pid in top_recommended_original:
            if pid in details_dict:
                print(f"- {details_dict[pid]}")

print('\n')

"""Sistem rekomendasi berhasil menganalisis preferensi User 75 berdasarkan 5 tempat favorit mereka (mix budaya-sejarah dan alam). 10 rekomendasi baru yang menunjukkan diversifikasi kategori - dari religious sites (La Kana Chapel), museum-monumen (budaya), hingga wisata alam (Curug Aseupan, Pemandian Ciater)."""